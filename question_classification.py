# -*- coding: utf-8 -*-
"""Question_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SqGSdFJYKjxgXkIQGrZDFHd1bFyoco0o
"""

!nvidia-smi

!pip install bert-for-tf2 >> /dev/null
!pip install sentencepiece >> /dev/null

import numpy as np
import os

import matplotlib
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import seaborn as sns
import copy 

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report
from sklearn.linear_model import LogisticRegression
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
import tensorflow_hub as hub
from tensorflow import keras

from gensim.models import Doc2Vec
from sklearn import utils
import gensim
from gensim.models.doc2vec import TaggedDocument
from gensim.models import doc2vec

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.metrics import classification_report,confusion_matrix

from sklearn.linear_model import SGDClassifier
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB

import spacy
nlp = spacy.load('en_core_web_sm')

# from nltk.util import ngrams
# from collections import defaultdict
# from collections import Counter
plt.style.use('ggplot')
# import nltk
# nltk.download('stopwords')
# from nltk.corpus import stopwords
# stopWordList=set(stopwords.words('english'))

import re
from nltk.tokenize import word_tokenize
import gensim
import string

import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
import tensorflow_hub as hub


from tqdm import tqdm
from tensorflow.keras.preprocessing.text import Tokenizer 
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import regularizers
from keras.models import Sequential
from keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout
from keras.initializers import Constant
from keras.optimizers import Adam

from sklearn import preprocessing

import bert 
from bert import BertModelLayer
from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights
from bert.tokenization.bert_tokenization import FullTokenizer

from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
from matplotlib import rc

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)
rcParams['figure.figsize'] = 12,8

"""# Load the data"""

df_train= pd.read_csv('train5.csv')
df_test=pd.read_csv('test.csv')

"""# Check the dataframe"""

df_train

"""# Check data distribution per category"""

chart = sns.countplot(df_train.category)
plt.title("Number of examples per category")

"""# Check the distribution of the questions' length"""

df_train['length'] = df_train['question'].str.split().apply(len)
bins = 10
plt.hist(df_train[df_train['category'] == 'DESC']['length'], alpha = 0.6, bins=bins, label='DESC')
plt.hist(df_train[df_train['category'] == 'ENTY']['length'], alpha = 0.6, bins=bins, label='ENTY')
plt.hist(df_train[df_train['category'] == 'ABBR']['length'], alpha = 0.6, bins=bins, label='ABBR')
plt.hist(df_train[df_train['category'] == 'HUM']['length'], alpha = 0.6, bins=bins, label='HUM')
plt.hist(df_train[df_train['category'] == 'NUM']['length'], alpha = 0.6, bins=bins, label='NUM')
plt.hist(df_train[df_train['category'] == 'LOC']['length'], alpha = 0.6, bins=bins, label='LOC')
plt.xlabel('length')
plt.ylabel('frequency')
plt.legend(loc='upper left')
plt.xlim(0,40)
plt.grid()
plt.show()

"""## From the distribution it seems like the highest length of questions of any category is 35.

# Check for null values in the dataframe
"""

#check null values
df_train.isna().sum().sort_values(ascending=False)

"""There is no null values in the samples.

# Train-test split
"""

# Encoding labels
encoder = preprocessing.LabelEncoder()
encoder.fit(df_train['category'])
train_label = encoder.transform(df_train['category'])
test_label = encoder.transform(df_test['category'])
df_train['category_en'] = train_label
df_test['category_en'] = test_label

# train-val split
text = df_train["question"].tolist()
labels = train_label
X_train, X_val, y_train, y_val = train_test_split(text, labels, test_size=0.2, random_state=RANDOM_SEED)

#test data
X_test = df_test['question']
y_test = df_test['category']

# Check the data
df_train.head(20)

"""# Helper function for performance and confusion matrix analysis"""

# Classificaiton metrics - precision, recall, f1, and accuracy
def get_metrics(y_test, y_predicted):  
    # true positives / (true positives+false positives)
    precision = precision_score(y_test, y_predicted, pos_label=None,
                                    average='weighted')             
    # true positives / (true positives + false negatives)
    recall = recall_score(y_test, y_predicted, pos_label=None,
                              average='weighted')
    
    # harmonic mean of precision and recall
    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')
    
    # true positives + true negatives/ total
    accuracy = accuracy_score(y_test, y_predicted)
    return accuracy, precision, recall, f1

# Showing Confusion Matrix
def plot_cm(y_true, y_pred, title, figsize=(5,5)):
    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))
    cm_sum = np.sum(cm, axis=1, keepdims=True)
    cm_perc = cm / cm_sum.astype(float) * 100
    annot = np.empty_like(cm).astype(str)
    nrows, ncols = cm.shape
    for i in range(nrows):
        for j in range(ncols):
            c = cm[i, j]
            p = cm_perc[i, j]
            if i == j:
                s = cm_sum[i]
                annot[i, j] = '%.1f%%\n%d/%d' % (p, c, s)
            elif c == 0:
                annot[i, j] = ''
            else:
                annot[i, j] = '%.1f%%\n%d' % (p, c)
    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))
    cm.index.name = 'Actual'
    cm.columns.name = 'Predicted'
    fig, ax = plt.subplots(figsize=figsize)
    plt.title(title)
    sns.heatmap(cm, cmap= "YlGnBu", annot=annot, fmt='', ax=ax)

# Ploting loss and accuracy graphs
def plot_graphs(f, history, string):
    plt.plot(history.history[string])
    plt.plot(history.history['val_'+string])
    plt.xlabel("Epochs")
    plt.ylabel(string)
    plt.legend([string, 'val_'+string])
    plt.title("Training "+ string+ " VS "+ "Validation "+string)
    plt.show()

"""# Tf-Idf + SVM Model"""

# Define model pipeline
sgd = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', SGDClassifier(loss='hinge', penalty='l1',alpha=1e-3, random_state=RANDOM_SEED, max_iter=5, tol=None)),
               ])

sgd.fit(X_train, y_train)

y_pred_svm = sgd.predict(X_test)
accuracy, precision, recall, f1 = get_metrics(df_test.category_en, y_pred_svm)
print("accuracy = %.3f \nprecision = %.3f \nrecall = %.3f \nf1 = %.3f \n" % (accuracy, precision, recall, f1))

# decode the label encoded results
test_pred_decode = encoder.inverse_transform(y_pred_svm)
# Showing Confusion Matrix for SVM model
plot_cm(test_pred_decode, df_test.category, 'Confusion matrix for SVM model', figsize=(7,7))

# Save results
data = {'model': 'Tf-Idf + SVM', 'precision': precision, 'recall': recall, 'f1': f1, 'accuracy': accuracy}
df_performance = pd.DataFrame(data, index=[0])

"""# Doc2Vec + Logistic Regression"""

# helper moethod to preprocess text according to the Doc2Vec model
def create_corpus_d2v(df):
    """
    The format will be ["word tokens", "label"]
    """
    #Tag train set
  
    labeled = []
    for i, v in enumerate(tqdm(df['question'])):
        label = df['category_en'][i]
        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))
    return labeled

X_train_lr = create_corpus_d2v(df_train)
X_test_lr = create_corpus_d2v(df_test)

X_train_lr[:5]

# Define doc2vec 
model_d2v = Doc2Vec( 
    dm=0, # 0: PV-DBOW
    vector_size=100, 
    negative=5, 
    min_count=2, 
    alpha=0.065, 
    min_alpha=0.065)

# Build vocabulary with only training samples
model_d2v.build_vocab([x for x in tqdm(X_train_lr)])

train_documents  = utils.shuffle(X_train_lr)
test_documents = X_test_lr

# train the d2v model
model_d2v.train(train_documents,total_examples=len(train_documents), epochs=300)

# Save model for future use
model_d2v.save('./d2vModel.d2v')

def vector_for_d2v(model, input_docs):
    sents = input_docs
    targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=30)) for doc in sents])
    return targets, feature_vectors

y_train_d2v, X_train_d2v = vector_for_d2v(model_d2v, train_documents)
y_test_d2v, X_test_d2v = vector_for_d2v(model_d2v, test_documents)

# Check the d2v embedding 
X_train_d2v[:5]

# Ref: https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb
# Using LSA - Latent Semantic Analysis to visualize the vectors in 2d space
def plot_LSA(test_data, test_labels, savepath="PCA_demo.csv", plot=True):
  lsa = TruncatedSVD(n_components=2) # Singular Value Decomposition
  lsa.fit(test_data)
  lsa_scores = lsa.transform(test_data)
  color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}
  color_column = [color_mapper[label] for label in test_labels]
  colors = ['red','blue','black', 'yellow', 'green', 'purple']
  if plot:
    plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=50, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))
    red_patch = mpatches.Patch(color='red', label='ABBR')
    blue_patch = mpatches.Patch(color='blue', label='DESC')
    black_patch = mpatches.Patch(color='black', label='ENTY')
    yellow_patch = mpatches.Patch(color='yellow', label='HUM')
    green_patch = mpatches.Patch(color='green', label='LOC')
    purple_patch = mpatches.Patch(color='purple', label='NUM')
    plt.title('Visualize Doc2Vec Emdedding', fontsize=20)
    plt.legend(handles=[red_patch, blue_patch, black_patch, yellow_patch, green_patch, purple_patch], prop={'size': 20})

fig = plt.figure(figsize=(16, 16))          
plot_LSA(X_train_d2v, y_train_d2v)
plt.show()

"""### The embeddings look quite separated in 2D space. Let's see if this leads to better performance."""

# Define logistic regression model
logreg = LogisticRegression(C=5, multi_class='multinomial', solver='saga',max_iter=1000)
logreg.fit(X_train_d2v, y_train_d2v)
y_pred_lr = logreg.predict(X_test_d2v)

accuracy, precision, recall, f1 = get_metrics(df_test.category_en, y_pred_lr)
print("accuracy = %.3f \nprecision = %.3f \nrecall = %.3f \nf1 = %.3f \n" % (accuracy, precision, recall, f1))

# decode the label encoded results
test_pred_decode = encoder.inverse_transform(y_pred_lr)
# Showing Confusion Matrix for LR model
plot_cm(test_pred_decode, df_test.category, 'Confusion matrix for Logistic Regression model', figsize=(7,7))

# Save results
data = {'model': 'Doc2Vec + Logistic Regression', 'precision': precision, 'recall': recall, 'f1': f1, 'accuracy': accuracy}
df_performance = df_performance.append(data, ignore_index=True)

"""# GloVe + LSTM Model"""

def create_corpus_new(df):
    corpus=[]
    for q in tqdm(df['question']):
        sentence = nlp(q)
        words = [i.text for i in sentence]
        corpus.append(words)
    return corpus   
corpus_train=create_corpus_new(df_train)
corpus_test = create_corpus_new(df_test)

# read the glove embedding and create a dict
embedding_dict={}
with open('glove.6B.100d.txt','r') as f:
    for line in f:
        values=line.split()
        word = values[0]
        vectors=np.asarray(values[1:],'float32')
        embedding_dict[word]=vectors
f.close()

# we have seen from the EDA that the max length available in the tweets is 35
MAX_LEN = 35
tokenizer_obj=Tokenizer()
tokenizer_obj.fit_on_texts(corpus_train)
sequences_train=tokenizer_obj.texts_to_sequences(corpus_train)
sequences_test =tokenizer_obj.texts_to_sequences(corpus_test)

question_pad_train=pad_sequences(sequences_train,maxlen=MAX_LEN,truncating='post',padding='post')
question_pad_test=pad_sequences(sequences_test,maxlen=MAX_LEN,truncating='post',padding='post')

word_index=tokenizer_obj.word_index
num_words=len(word_index)+1

# defining the embedding matrix
embedding_matrix=np.zeros((num_words,100))
for word,i in tqdm(word_index.items()):
    if i < num_words:
        emb_vec=embedding_dict.get(word)
        if emb_vec is not None:
            embedding_matrix[i]=emb_vec

# Check the embedding after processing
embedding_matrix[-5:]

#train-val split
X_train, X_val, y_train, y_val = train_test_split(question_pad_train, df_train['category_en'].values, test_size=0.2, random_state=1)

print('Shape of train',X_train.shape)
print("Shape of Validation ",X_val.shape)

model=Sequential()

embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),
                   input_length=MAX_LEN,trainable=False)

model.add(embedding)
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(6, activation='softmax'))


model.compile(Adam(lr=1e-2), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

history_lstm = model.fit(X_train,y_train,batch_size=32,epochs=10,validation_data=(X_val,y_val),verbose=1)

f1 = plt.figure()
plot_graphs(f1,history_lstm, "accuracy")

f2 = plt.figure()
plot_graphs(f2,history_lstm, "loss")

y_pred_lstm = model.predict(question_pad_test).argmax(axis=-1)
accuracy, precision, recall, f1 = get_metrics(df_test.category_en, y_pred_lstm)
print("accuracy = %.3f \nprecision = %.3f \nrecall = %.3f \nf1 = %.3f \n" % (accuracy, precision, recall, f1))

# decode the label encoded results
test_pred_decode = encoder.inverse_transform(y_pred_lstm)
# Showing Confusion Matrix for LSTM model
plot_cm(test_pred_decode, df_test.category, 'Confusion matrix for LSTM-Glove model', figsize=(7,7))

# Save results
data = {'model': 'GloVe + LSTM', 'precision': precision, 'recall': recall, 'f1': f1, 'accuracy': accuracy}
df_performance = df_performance.append(data, ignore_index=True)

"""# BERT"""

def bert_encode(texts, tokenizer, max_len=512):
    all_tokens = []
    all_masks = []
    all_segments = []
    
    for text in texts:
        text = tokenizer.tokenize(text)
            
        text = text[:max_len-2]
        input_sequence = ["[CLS]"] + text + ["[SEP]"]
        pad_len = max_len - len(input_sequence)
        
        tokens = tokenizer.convert_tokens_to_ids(input_sequence)
        tokens += [0] * pad_len
        pad_masks = [1] * len(input_sequence) + [0] * pad_len
        segment_ids = [0] * max_len
        
        all_tokens.append(tokens)
        all_masks.append(pad_masks)
        all_segments.append(segment_ids)
    
    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)

def build_model(bert_layer, max_len=512):
    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    input_mask = Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name="segment_ids")

    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])
    clf_output = sequence_output[:, 0, :]
    out = Dense(6, activation='softmax')(clf_output)
    
    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)
    model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    return model, clf_output

# load it from local drive
import shutil
shutil.unpack_archive("bert_en_uncased_L-24_H-1024_A-16_1.tar.gz", "./bert_en_uncased_L-24_H-1024_A-16_1/")
model_path = "bert_en_uncased_L-24_H-1024_A-16_1"
# Download BERT layer from tfhub
# model_path = "https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1"
bert_layer = hub.KerasLayer(model_path, trainable=True)

train = df_train
test = df_test

# Google tokenizer download
# !wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py

#load tokenizer, vocab, the tokenization is the file named as tokenization.py 
#the Tokenization.py file
import tokenization
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)

#prepare the training data and encode data into tokens, masks and segment flags
train_input = bert_encode(train.question.values, tokenizer, max_len=35)
test_input = bert_encode(test.question.values, tokenizer, max_len=35)
train_labels = train.category_en.values

# Build the model
model,clf_output = build_model(bert_layer, max_len=35)
model.summary()

#train the model
checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)

history_bert = model.fit(
    train_input, train_labels,
    validation_split=0.2,
    epochs=5,
    callbacks=[checkpoint],
    batch_size=16
)

f1 = plt.figure()
plot_graphs(f1,history_bert, "accuracy")

f2 = plt.figure()
plot_graphs(f2,history_bert, "loss")

#prediction of test data 
# Uncomment if loading the saved model
# model.load_weights('model.h5')
y_pred = model.predict(test_input).argmax(axis=-1)

#model evaluation
accuracy, precision, recall, f1 = get_metrics(test.category_en,y_pred)
print("accuracy = %.3f \nprecision = %.3f \nrecall = %.3f \nf1 = %.3f" % (accuracy, precision, recall, f1))

# decode the label encoded results
test_pred_decode = encoder.inverse_transform(y_pred)
# Showing Confusion Matrix for bert model
plot_cm(df_test.category, test_pred_decode, 'Confusion matrix for BERT model', figsize=(7,7))

# Save results
data = {'model': 'BERT', 'precision': precision, 'recall': recall, 'f1': f1, 'accuracy': accuracy}
df_performance = df_performance.append(data, ignore_index=True)

"""# Performance Analysis"""

df_performance